{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "04_Backpropagation.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "b2baa5b2165748cb99254caa87a0598e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_1b23fd1ea29b4471a8664b40b42522a5",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_50bb9f2e3738449aaaf54cb0066b814d",
              "IPY_MODEL_284cfff14789460b83757a6ae4db5133"
            ]
          }
        },
        "1b23fd1ea29b4471a8664b40b42522a5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "50bb9f2e3738449aaaf54cb0066b814d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "IntProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_0cd1eb86681b40269f817573f66c047a",
            "_dom_classes": [],
            "description": "",
            "_model_name": "IntProgressModel",
            "bar_style": "success",
            "max": 1,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_2a4cf59273a247668b900f68825cbde1"
          }
        },
        "284cfff14789460b83757a6ae4db5133": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_193ad122822840abaf1f94dc2fec10ca",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "170500096it [00:04, 41968956.37it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_94aaa41536524f53a6d6da093be11cca"
          }
        },
        "0cd1eb86681b40269f817573f66c047a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "2a4cf59273a247668b900f68825cbde1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "193ad122822840abaf1f94dc2fec10ca": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "94aaa41536524f53a6d6da093be11cca": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bghadge/Computer-Vision/blob/master/04_Backpropagation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ix5dQS2rUMlu",
        "colab_type": "text"
      },
      "source": [
        "#EECS 504 PS4: Backpropagation\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W_Cst4k4tuBc",
        "colab_type": "text"
      },
      "source": [
        "# Starting\n",
        "\n",
        "Run the following code to import the modules you'll need. After your finish the assignment, remember to run all cells and save the note book to your local machine as a .ipynb file for Canvas submission."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SHumIO-xt57H",
        "colab_type": "code",
        "outputId": "93298dac-5cea-4de8-da90-b74a29c32dc2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 100,
          "referenced_widgets": [
            "b2baa5b2165748cb99254caa87a0598e",
            "1b23fd1ea29b4471a8664b40b42522a5",
            "50bb9f2e3738449aaaf54cb0066b814d",
            "284cfff14789460b83757a6ae4db5133",
            "0cd1eb86681b40269f817573f66c047a",
            "2a4cf59273a247668b900f68825cbde1",
            "193ad122822840abaf1f94dc2fec10ca",
            "94aaa41536524f53a6d6da093be11cca"
          ]
        }
      },
      "source": [
        "import pickle\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "from torchvision.datasets import CIFAR10\n",
        "download = not os.path.isdir('cifar-10-batches-py')\n",
        "dset_train = CIFAR10(root='.', download=download)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./cifar-10-python.tar.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b2baa5b2165748cb99254caa87a0598e",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Extracting ./cifar-10-python.tar.gz to .\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "apEPzDNtK0MC",
        "colab_type": "text"
      },
      "source": [
        "# Problem 4.2 Multi-layer perceptron\n",
        "In this problem you will develop a two Layer neural network with fully-connected layers to perform classification, and test it out on the CIFAR-10 dataset.\n",
        "\n",
        "We train the network with a softmax loss function on the weight matrices. The network uses a ReLU nonlinearity after the first fully connected layer. In other words, the network has the following architecture:\n",
        "\n",
        "input - fully connected layer - ReLU - fully connected layer - softmax\n",
        "\n",
        "The outputs of the second fully-connected layer are the scores for each class.\n",
        "\n",
        "You cannot use any deep learning libraries such as PyTorch in this part."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SXfumCQ21JoK",
        "colab_type": "text"
      },
      "source": [
        "# 4.2 (a) Layers\n",
        "In this problem, implement fully connected layer, relu and softmax. Filling in all TODOs in skeleton codes will be sufficient."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q-ljfgMv9PHx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def fc_forward(x, w, b):\n",
        "    \"\"\"\n",
        "    Computes the forward pass for a fully-connected layer.\n",
        "    \n",
        "    The input x has shape (N, Din) and contains a minibatch of N\n",
        "    examples, where each example x[i] has shape (Din,).\n",
        "    \n",
        "    Inputs:\n",
        "    - x: A numpy array containing input data, of shape (N, Din)\n",
        "    - w: A numpy array of weights, of shape (Din, Dout)\n",
        "    - b: A numpy array of biases, of shape (Dout,)\n",
        "    \n",
        "    Returns a tuple of:\n",
        "    - out: output, of shape (N, Dout)\n",
        "    - cache: (x, w, b)\n",
        "    \"\"\"\n",
        "    out = x@w + b\n",
        "    cache = (x, w, b)\n",
        "    return out, cache\n",
        "\n",
        "\n",
        "def fc_backward(dout, cache):\n",
        "    \"\"\"\n",
        "    Computes the backward pass for a fully_connected layer.\n",
        "    \n",
        "    Inputs:\n",
        "    - dout: Upstream derivative, of shape (N, Dout)\n",
        "    - cache: returned by your forward function. Tuple of:\n",
        "      - x: Input data, of shape (N, Din)\n",
        "      - w: Weights, of shape (Din, Dout)\n",
        "      - b: Biases, of shape (Dout,)\n",
        "      \n",
        "    Returns a tuple of:\n",
        "    - dx: Gradient with respect to x, of shape (N, Din)\n",
        "    - dw: Gradient with respect to w, of shape (Din, Dout)\n",
        "    - db: Gradient with respect to b, of shape (Dout,)\n",
        "    \"\"\"\n",
        "    x, w, b = cache\n",
        "    dx, dw, db = None, None, None\n",
        "    dx = dout @ np.transpose(w)\n",
        "    dw = np.transpose(x) @ dout\n",
        "    db = np.transpose(dout) @ np.ones(dout.shape[0])\n",
        "    return dx, dw, db\n",
        "\n",
        "def relu_forward(x):\n",
        "    \"\"\"\n",
        "    Computes the forward pass for a layer of rectified linear units (ReLUs).\n",
        "\n",
        "    Input:\n",
        "    - x: Inputs, of any shape\n",
        "\n",
        "    Returns a tuple of:\n",
        "    - out: Output, of the same shape as x\n",
        "    - cache: x\n",
        "    \"\"\"\n",
        "    out = x\n",
        "    out = np.where(x<0,0,x)\n",
        "    cache = x\n",
        "    return out, cache\n",
        "\n",
        "\n",
        "def relu_backward(dout, cache):\n",
        "    \"\"\"\n",
        "    Computes the backward pass for a layer of rectified linear units (ReLUs).\n",
        "\n",
        "    Input:\n",
        "    - dout: Upstream derivatives, of any shape\n",
        "    - cache: returned by your forward function. Input x, of same shape as dout\n",
        "\n",
        "    Returns:\n",
        "    - dx: Gradient with respect to x\n",
        "    \"\"\"\n",
        "    dx, x = dout, cache\n",
        "    dx = dout * (x >= 0)\n",
        "    return dx\n",
        "\n",
        "\n",
        "def softmax_loss(x, y):\n",
        "    \"\"\"\n",
        "    Computes the loss and gradient for softmax classification.\n",
        "\n",
        "    Inputs:\n",
        "    - x: Input data, of shape (N, C) where x[i, j] is the score for the jth\n",
        "      class for the ith input.\n",
        "    - y: Vector of labels, of shape (N,) where y[i] is the label for x[i] and\n",
        "      0 <= y[i] < C\n",
        "\n",
        "    Returns a tuple of:\n",
        "    - loss: Scalar giving the loss\n",
        "    - dx: Gradient of the loss with respect to x\n",
        "    \"\"\"\n",
        "    loss, dx = None, None\n",
        "    e_power = np.exp(np.transpose((np.transpose(x) - np.max(np.transpose(x),0))))\n",
        "    sum_e_power = np.sum(e_power,1)\n",
        "    softmax = (e_power / sum_e_power[:,None])\n",
        "    log_likelihood = -np.log(softmax[range(y.shape[0]),y])\n",
        "    loss = np.sum(log_likelihood) / y.shape[0]\n",
        "    dx = softmax\n",
        "    dx[range(y.shape[0]),y] -= 1\n",
        "    dx = dx/y.shape[0]\n",
        "    return loss, dx"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LbFxtS3zK8oz",
        "colab_type": "text"
      },
      "source": [
        "# 4.2 (b) Softmax Classifier\n",
        "\n",
        "In this problem, implement softmax classifier."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ytvxbx9UpxVL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class SoftmaxClassifier(object):\n",
        "    \"\"\"\n",
        "    A fully-connected neural network with\n",
        "    softmax loss that uses a modular layer design. We assume an input dimension\n",
        "    of D, a hidden dimension of H, and perform classification over C classes.\n",
        "\n",
        "    The architecture should be fc - relu - fc - softmax with one hidden layer\n",
        "\n",
        "    The learnable parameters of the model are stored in the dictionary\n",
        "    self.params that maps parameter names to numpy arrays.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, input_dim=3072, hidden_dim=300, num_classes=10,\n",
        "                 weight_scale=1e-3):\n",
        "        \"\"\"\n",
        "        Initialize a new network.\n",
        "\n",
        "        Inputs:\n",
        "        - input_dim: An integer giving the size of the input\n",
        "        - hidden_dim: An integer giving the size of the hidden layer, None\n",
        "          if there's no hidden layer.\n",
        "        - num_classes: An integer giving the number of classes to classify\n",
        "        - weight_scale: Scalar giving the standard deviation for random\n",
        "          initialization of the weights.\n",
        "        \"\"\"\n",
        "        self.params = {}\n",
        "        ############################################################################\n",
        "        # TODO: Initialize the weights and biases of the two-layer net. Weights    #\n",
        "        # should be initialized from a Gaussian centered at 0.0 with               #\n",
        "        # standard deviation equal to weight_scale, and biases should be           #\n",
        "        # initialized to zero. All weights and biases should be stored in the      #\n",
        "        # dictionary self.params, with fc weights and biases using the keys        #\n",
        "        # 'W' and 'b', i.e., W1, b1 for the weights and bias in the first linear   #\n",
        "        # layer, W2, b2 for the weights and bias in the second linear layer.       #\n",
        "        ############################################################################\n",
        "        w1 = np.random.normal(loc=0.0,scale=weight_scale,size=[input_dim,hidden_dim])\n",
        "        b1 = np.zeros(hidden_dim)\n",
        "        w2 = np.random.normal(loc=0.0,scale=weight_scale,size=[hidden_dim, num_classes])\n",
        "        b2 = np.zeros(num_classes)\n",
        "        self.params.update({\"w1\":w1, \"b1\":b1, \"w2\":w2,  \"b2\":b2})\n",
        "        self.hidden_dim = hidden_dim\n",
        "\n",
        "    def forwards_backwards(self, X, y=None):\n",
        "        \"\"\"\n",
        "        Compute loss and gradient for a minibatch of data.\n",
        "\n",
        "        Inputs:\n",
        "        - X: Array of input data of shape (N, Din)\n",
        "        - y: Array of labels, of shape (N,). y[i] gives the label for X[i].\n",
        "\n",
        "        Returns:\n",
        "        If y is None, then run a test-time forward pass of the model and return:\n",
        "        - scores: Array of shape (N, C) giving classification scores, where\n",
        "          scores[i, c] is the classification score for X[i] and class c.\n",
        "\n",
        "        If y is not None, then run a training-time forward and backward pass. And\n",
        "        return a tuple of:\n",
        "        - loss: Scalar value giving the loss\n",
        "        - grads: Dictionary with the same keys as self.params, mapping parameter\n",
        "          names to gradients of the loss with respect to those parameters.\n",
        "        \"\"\"\n",
        "        scores = None\n",
        "        z1, z1_cache = fc_forward(X, self.params[\"w1\"], self.params[\"b1\"])\n",
        "        a1, a1_cache = relu_forward(z1)\n",
        "        z2, z2_cache = fc_forward(a1, self.params[\"w2\"], self.params[\"b2\"])\n",
        "        scores = z2\n",
        "        \n",
        "        # If y is None then we are in test mode so just return scores\n",
        "        if y is None:\n",
        "            return scores\n",
        "\n",
        "        loss, grads = 0, {}\n",
        "        ############################################################################\n",
        "        # TODO: Implement the backward pass for the two-layer net. Store the loss  #\n",
        "        # in the loss variable and gradients in the grads dictionary. Compute data #\n",
        "        # loss using softmax, and make sure that grads[k] holds the gradients for  #\n",
        "        # self.params[k].                                                          # \n",
        "        ############################################################################\n",
        "        loss, dL_dz2 = softmax_loss(scores,y)\n",
        "        (dL_da1, dL_dw2, dL_db2) = fc_backward(dL_dz2, z2_cache)\n",
        "        dL_dz1 = relu_backward(dL_da1, a1_cache)\n",
        "        (dL_dx1, dL_dw1, dL_db1) = fc_backward(dL_dz1, z1_cache)\n",
        "        grads.update({\"w1\":dL_dw1, \"w2\":dL_dw2, \"b1\":dL_db1, \"b2\":dL_db2})\n",
        "        return loss, grads\n",
        "\n",
        "  \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lwp0waIL1h_e",
        "colab_type": "text"
      },
      "source": [
        "# 4.2(c) Training\n",
        "\n",
        "In this problem, you need to preprocess the images and set up model hyperparameters. Notice that adjust the training and val split is optional."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kZPtQzXGMoCg",
        "colab_type": "code",
        "outputId": "205d1064-a277-4014-cd98-739ac77f872d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        }
      },
      "source": [
        "def unpickle(file):\n",
        "    with open(file, 'rb') as fo:\n",
        "        dict = pickle.load(fo, encoding=\"latin1\")\n",
        "    return dict\n",
        "\n",
        "def load_cifar10():\n",
        "    data = {}\n",
        "    meta = unpickle(\"cifar-10-batches-py/batches.meta\")\n",
        "    batch1 = unpickle(\"cifar-10-batches-py/data_batch_1\")\n",
        "    batch2 = unpickle(\"cifar-10-batches-py/data_batch_2\")\n",
        "    batch3 = unpickle(\"cifar-10-batches-py/data_batch_3\")\n",
        "    batch4 = unpickle(\"cifar-10-batches-py/data_batch_4\")\n",
        "    batch5 = unpickle(\"cifar-10-batches-py/data_batch_5\")\n",
        "    test_batch = unpickle(\"cifar-10-batches-py/test_batch\")\n",
        "    X_train = np.vstack((batch1['data'], batch2['data'], batch3['data'],\\\n",
        "                         batch4['data'], batch5['data']))\n",
        "    Y_train = np.array(batch1['labels'] + batch2['labels'] + batch3['labels'] + \n",
        "                       batch4['labels'] + batch5['labels'])\n",
        "    X_test = test_batch['data']\n",
        "    Y_test = test_batch['labels']\n",
        "    \n",
        "    #Preprocess images here                                     \n",
        "    X_train = (X_train-np.mean(X_train,axis=1,keepdims=True))/np.std(X_train,axis=1,keepdims=True)\n",
        "    X_test = (X_test-np.mean(X_test,axis=1,keepdims=True))/np.std(X_test,axis=1,keepdims=True)\n",
        "\n",
        "    data['X_train'] = X_train[:40000]\n",
        "    data['y_train'] = Y_train[:40000]\n",
        "    data['X_val'] = X_train[40000:]\n",
        "    data['y_val'] = Y_train[40000:]\n",
        "    data['X_test'] = X_test\n",
        "    data['y_test'] = Y_test\n",
        "    return data\n",
        "\n",
        "def test_network(model, X, y, num_samples=None, batch_size=100):\n",
        "    \"\"\"\n",
        "    Check accuracy of the model on the provided data.\n",
        "\n",
        "    Inputs:\n",
        "    - model: Image classifier\n",
        "    - X: Array of data, of shape (N, d_1, ..., d_k)\n",
        "    - y: Array of labels, of shape (N,)\n",
        "    - num_samples: If not None, subsample the data and only test the model\n",
        "      on num_samples datapoints.\n",
        "    - batch_size: Split X and y into batches of this size to avoid using\n",
        "      too much memory.\n",
        "\n",
        "    Returns:\n",
        "    - acc: Scalar giving the fraction of instances that were correctly\n",
        "      classified by the model.\n",
        "    \"\"\"\n",
        "\n",
        "    # Subsample the data\n",
        "    N = X.shape[0]\n",
        "    if num_samples is not None and N > num_samples:\n",
        "        mask = np.random.choice(N, num_samples)\n",
        "        N = num_samples\n",
        "        X = X[mask]\n",
        "        y = y[mask]\n",
        "\n",
        "    # Compute predictions in batches\n",
        "    num_batches = N // batch_size\n",
        "    if N % batch_size != 0:\n",
        "        num_batches += 1\n",
        "    y_pred = []\n",
        "    for i in range(num_batches):\n",
        "        start = i * batch_size\n",
        "        end = (i + 1) * batch_size\n",
        "        scores = model.forwards_backwards(X[start:end])\n",
        "        y_pred.append(np.argmax(scores, axis=1))\n",
        "    y_pred = np.hstack(y_pred)\n",
        "    acc = np.mean(y_pred == y)\n",
        "\n",
        "    return acc\n",
        "\n",
        "\n",
        "def train_network(model, data, **kwargs):\n",
        "    \"\"\"\n",
        "     Required arguments:\n",
        "    - model: Image classifier\n",
        "    - data: A dictionary of training and validation data containing:\n",
        "      'X_train': Array, shape (N_train, d_1, ..., d_k) of training images\n",
        "      'X_val': Array, shape (N_val, d_1, ..., d_k) of validation images\n",
        "      'y_train': Array, shape (N_train,) of labels for training images\n",
        "      'y_val': Array, shape (N_val,) of labels for validation images\n",
        "\n",
        "    Optional arguments:\n",
        "    - learning_rate: A scalar for initial learning rate.\n",
        "    - lr_decay: A scalar for learning rate decay; after each epoch the\n",
        "      learning rate is multiplied by this value.\n",
        "    - batch_size: Size of minibatches used to compute loss and gradient\n",
        "      during training.\n",
        "    - num_epochs: The number of epochs to run for during training.\n",
        "    - print_every: Integer; training losses will be printed every\n",
        "      print_every iterations.\n",
        "    - verbose: Boolean; if set to false then no output will be printed\n",
        "      during training.\n",
        "    - num_train_samples: Number of training samples used to check training\n",
        "      accuracy; default is 1000; set to None to use entire training set.\n",
        "    - num_val_samples: Number of validation samples to use to check val\n",
        "      accuracy; default is None, which uses the entire validation set.\n",
        "    \"\"\"\n",
        "    \n",
        "    \n",
        "    learning_rate =  kwargs.pop('learning_rate', 1e-3)\n",
        "    lr_decay = kwargs.pop('lr_decay', 1.0)\n",
        "    batch_size = kwargs.pop('batch_size', 100)\n",
        "    num_epochs = kwargs.pop('num_epochs', 10)\n",
        "    num_train_samples = kwargs.pop('num_train_samples', 1000)\n",
        "    num_val_samples = kwargs.pop('num_val_samples', None)\n",
        "    print_every = kwargs.pop('print_every', 10)   \n",
        "    verbose = kwargs.pop('verbose', True)\n",
        "    \n",
        "    epoch = 0\n",
        "    best_val_acc = 0\n",
        "    best_params = {}\n",
        "    loss_history = []\n",
        "    train_acc_history = []\n",
        "    val_acc_history = []\n",
        "    \n",
        "    \n",
        "    num_train = data['X_train'].shape[0]\n",
        "    iterations_per_epoch = max(num_train // batch_size, 1)\n",
        "    num_iterations = num_epochs * iterations_per_epoch\n",
        "    \n",
        "\n",
        "    \n",
        "    for t in range(num_iterations):\n",
        "        # Make a minibatch of training data\n",
        "        batch_mask = np.random.choice(num_train, batch_size)\n",
        "        X_batch = data['X_train'][batch_mask]\n",
        "        y_batch = data['y_train'][batch_mask]\n",
        "        \n",
        "        # Compute loss and gradient\n",
        "        loss, grads = model.forwards_backwards(X_batch, y_batch)\n",
        "        loss_history.append(loss)\n",
        "\n",
        "        # Perform a parameter update\n",
        "        for p, w in model.params.items():\n",
        "            model.params[p] = w - grads[p]*learning_rate\n",
        "          \n",
        "        # Print training loss\n",
        "        if verbose and t % print_every == 0:\n",
        "            print('(Iteration %d / %d) loss: %f' % (\n",
        "                   t + 1, num_iterations, loss_history[-1]))\n",
        "         \n",
        "        # At the end of every epoch, increment the epoch counter and decay\n",
        "        # the learning rate.\n",
        "        epoch_end = (t + 1) % iterations_per_epoch == 0\n",
        "        if epoch_end:\n",
        "            epoch += 1\n",
        "            learning_rate *= lr_decay\n",
        "        \n",
        "        # Check train and val accuracy on the first iteration, the last\n",
        "        # iteration, and at the end of each epoch.\n",
        "        first_it = (t == 0)\n",
        "        last_it = (t == num_iterations - 1)\n",
        "        if first_it or last_it or epoch_end:\n",
        "            train_acc = test_network(model, data['X_train'], data['y_train'],\n",
        "                num_samples= num_train_samples)\n",
        "            val_acc = test_network(model, data['X_val'], data['y_val'],\n",
        "                num_samples=num_val_samples)\n",
        "            train_acc_history.append(train_acc)\n",
        "            val_acc_history.append(val_acc)\n",
        "\n",
        "            if verbose:\n",
        "                print('(Epoch %d / %d) train acc: %f; val_acc: %f' % (\n",
        "                       epoch, num_epochs, train_acc, val_acc))\n",
        "\n",
        "            # Keep track of the best model\n",
        "            if val_acc > best_val_acc:\n",
        "                best_val_acc = val_acc\n",
        "                best_params = {}\n",
        "                for k, v in model.params.items():\n",
        "                    best_params[k] = v.copy()\n",
        "        \n",
        "    model.params = best_params\n",
        "        \n",
        "    return model, train_acc_history, val_acc_history\n",
        "        \n",
        "\n",
        "# load data\n",
        "data = load_cifar10() \n",
        "train_data = { k: data[k] for k in ['X_train', 'y_train', \n",
        "                                    'X_val', 'y_val']}\n",
        "\n",
        "# initialize model\n",
        "model = SoftmaxClassifier(hidden_dim = 400, weight_scale=1e-2)\n",
        "\n",
        "# start training    \n",
        "model, train_acc_history, val_acc_history = train_network(\n",
        "    model, train_data, learning_rate =0.1 ,\n",
        "    lr_decay=0.06, num_epochs=10, \n",
        "    batch_size=100, print_every=1000)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(Iteration 1 / 4000) loss: 2.298753\n",
            "(Epoch 0 / 10) train acc: 0.158000; val_acc: 0.158400\n",
            "(Epoch 1 / 10) train acc: 0.487000; val_acc: 0.434400\n",
            "(Epoch 2 / 10) train acc: 0.523000; val_acc: 0.491400\n",
            "(Iteration 1001 / 4000) loss: 1.191464\n",
            "(Epoch 3 / 10) train acc: 0.543000; val_acc: 0.493000\n",
            "(Epoch 4 / 10) train acc: 0.539000; val_acc: 0.493300\n",
            "(Epoch 5 / 10) train acc: 0.538000; val_acc: 0.493300\n",
            "(Iteration 2001 / 4000) loss: 1.334416\n",
            "(Epoch 6 / 10) train acc: 0.536000; val_acc: 0.493300\n",
            "(Epoch 7 / 10) train acc: 0.558000; val_acc: 0.493300\n",
            "(Iteration 3001 / 4000) loss: 1.348497\n",
            "(Epoch 8 / 10) train acc: 0.545000; val_acc: 0.493300\n",
            "(Epoch 9 / 10) train acc: 0.546000; val_acc: 0.493300\n",
            "(Epoch 10 / 10) train acc: 0.554000; val_acc: 0.493300\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fcovGmpXvXXa",
        "colab_type": "text"
      },
      "source": [
        "# 4.2(c) Report Accuracy\n",
        "\n",
        "Run the given code and report the accuracy on test set.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FwCq8pBhu6dz",
        "colab_type": "code",
        "outputId": "d3442db9-d42c-4056-aa3d-78710d2ac1e9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# report test accuracy\n",
        "acc = test_network(model, data['X_test'], data['y_test'])\n",
        "print(\"Test accuracy: {}\".format(acc))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test accuracy: 0.4965\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oTrmbULS7i2N",
        "colab_type": "text"
      },
      "source": [
        "# 4.2(d) Plot\n",
        "\n",
        "Using the train_acc_history and val_acc_history, plot the train & val accuracy versus epochs on one plot. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SPjtnbya9S7g",
        "colab_type": "code",
        "outputId": "58d0ccc6-5c4a-494b-d1db-610739829f6b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 296
        }
      },
      "source": [
        "plt.figure()\n",
        "plt.plot(train_acc_history)\n",
        "plt.plot(val_acc_history)\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend(labels=['Training','Test'])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7f947b4aea20>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 42
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEGCAYAAAB/+QKOAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    }
  ]
}